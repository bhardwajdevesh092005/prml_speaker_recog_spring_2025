{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7178bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import librosa\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b80bd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.load(\"data/features/x_1_3.npy\")\n",
    "y=np.load(\"data/features/y_1_3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a28ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (117052, 43, 39)\n",
      "Test Data Shape: (29264, 43, 39)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Data Shape:\", x_train.shape)\n",
    "print(\"Test Data Shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape= (43,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7697de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpeakerResNet(nn.Module):\n",
    "    def __init__(self, input_shape, no_speakers, dropout_rate=0.5):\n",
    "        super(SpeakerResNet, self).__init__()\n",
    "        \n",
    "        self.time_frames, self.mfcc_features = input_shape\n",
    "\n",
    "        # Convolutional Block 1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Residual Block 1\n",
    "        self.res1_conv1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.res1_bn1 = nn.BatchNorm2d(64)\n",
    "        self.res1_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.res1_bn2 = nn.BatchNorm2d(64)\n",
    "        self.res1_shortcut = nn.Conv2d(32, 64, kernel_size=1)\n",
    "\n",
    "        # Residual Block 2\n",
    "        self.res2_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.res2_bn1 = nn.BatchNorm2d(128)\n",
    "        self.res2_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.res2_bn2 = nn.BatchNorm2d(128)\n",
    "        self.res2_shortcut = nn.Conv2d(64, 128, kernel_size=1)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(128, 512)\n",
    "        self.fc2 = nn.Linear(512, no_speakers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "        # Convolutional Block 1\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        # Residual Block 1\n",
    "        shortcut = self.res1_shortcut(x)\n",
    "        x = F.relu(self.res1_bn1(self.res1_conv1(x)))\n",
    "        x = self.res1_bn2(self.res1_conv2(x))\n",
    "        x += shortcut\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Residual Block 2\n",
    "        shortcut = self.res2_shortcut(x)\n",
    "        x = F.relu(self.res2_bn1(self.res2_conv1(x)))\n",
    "        x = self.res2_bn2(self.res2_conv2(x))\n",
    "        x += shortcut\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846f1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =SpeakerResNet(input_shape,no_speakers=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f2e00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeakerResNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res1_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (res1_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res1_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (res1_bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res1_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (res2_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (res2_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res2_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (res2_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (res2_shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04826e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e90894",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549f2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Early stopping setup\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpeakerResNet(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res1_conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (res1_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res1_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (res1_bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res1_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (res2_conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (res2_bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res2_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (res2_bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (res2_shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=51, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.2, 0.8)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    \n",
    "    # Model initialization with suggested dropout rate\n",
    "    model = SpeakerResNet(input_shape=(43, 39), no_speakers=51, dropout_rate=dropout_rate)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create DataLoader\n",
    "    train_dataset = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            preds = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    accuracy = accuracy_score(val_targets, val_preds)\n",
    "    return accuracy\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "# Save study results to a CSV file\n",
    "study_file = \"optuna_study_results_cnn.csv\"\n",
    "df = study.trials_dataframe()\n",
    "df.to_csv(study_file, index=False)\n",
    "print(f\"Study results saved to {study_file}\")\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = study.best_params\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model = SpeakerResNet(input_shape=(43, 39), no_speakers=50, dropout_rate=best_params[\"dropout_rate\"])\n",
    "final_model = final_model.to(device)\n",
    "optimizer = getattr(optim, best_params[\"optimizer\"])(final_model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Train the final model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate on the test set\n",
    "final_model.eval()\n",
    "test_preds = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        outputs = final_model(x_batch)\n",
    "        preds = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(test_accuracy * 100))\n",
    "# Save the final model\n",
    "torch.save(final_model.state_dict(), \"final_model_cnn.pth\")\n",
    "# Save the model architecture\n",
    "torch.save(final_model, \"final_model_architecture.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
